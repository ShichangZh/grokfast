{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_2752431/2491411562.py:2: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  results = torch.load('results/res_test_none.pt')\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "results = torch.load('results/res_test_none.pt')\n",
    "# results = torch.load('results/res_test_ma_w100_l5_wd10e-02.pt')\n",
    "# results.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "weights = results['net']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "\n",
    "from dattri.algorithm.influence_function import IFAttributor\n",
    "from dattri.benchmark.datasets.mnist import train_mnist_lr, create_mnist_dataset\n",
    "from dattri.func.utils import flatten_func\n",
    "from dattri.benchmark.utils import SubsetSampler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def multiplication_mod_p_data(p, eq_token, op_token):\n",
    "    \"\"\"x◦y = x/y (mod p) for 0 ≤ x < p, 0 < y < p\n",
    "    \"\"\"\n",
    "    x = torch.arange(p)\n",
    "    y = torch.arange(1, p)\n",
    "    x, y = torch.cartesian_prod(x, y).T\n",
    "\n",
    "    eq = torch.ones_like(x) * eq_token\n",
    "    op = torch.ones_like(x) * op_token\n",
    "    result = x * y % p\n",
    "\n",
    "    # \"All of our experiments used a small transformer trained on datasets of\n",
    "    # equations of the form a◦b = c, where each of “a”, “◦”, “b”, “=”, and “c”\n",
    "    # is a seperate token\"\n",
    "    return torch.stack([x, op, y, eq, result])\n",
    "\n",
    "from argparse import ArgumentParser\n",
    "parser = ArgumentParser()\n",
    "parser.add_argument(\"--p\", type=int, default=97)\n",
    "args = parser.parse_args('')\n",
    "\n",
    "\n",
    "eq_token = args.p\n",
    "op_token = args.p + 1\n",
    "\n",
    "data = multiplication_mod_p_data(args.p, eq_token, op_token)\n",
    "\n",
    "train_idx, valid_idx = torch.randperm(data.shape[1]).split(data.shape[1] // 2)\n",
    "train_data, valid_data = data[:, train_idx], data[:, valid_idx]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "139"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, dataset_test = create_mnist_dataset(\"./data\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_train, dataset_test = create_mnist_dataset(\"./data\")\n",
    "\n",
    "# Algorithmic data\n",
    "dataset_train = train_data.T\n",
    "dataset_test = valid_data.T\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset_train,\n",
    "    batch_size=64,\n",
    "    sampler=SubsetSampler(range(1000)),\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset_test,\n",
    "    batch_size=64,\n",
    "    sampler=SubsetSampler(range(100)),\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([64, 5])"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from main import Decoder\n",
    "device = torch.device(\"cuda:7\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "model = Decoder(\n",
    "    dim=128, num_layers=2, num_heads=4, num_tokens=args.p + 2, seq_len=5\n",
    ").to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(weights[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                               "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[10, 98,  8, 97, 80]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[12, 98, 55, 97, 78]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 9, 98,  7, 97, 63]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[77, 98, 10, 97, 91]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[65, 98,  9, 97,  3]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[29, 98, 12, 97, 57]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[33, 98, 29, 97, 84]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[26, 98, 57, 97, 27]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[14, 98, 23, 97, 31]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[43, 98, 52, 97,  5]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[19, 98, 83, 97, 25]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[56, 98, 47, 97, 13]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[44, 98, 42, 97,  5]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[95, 98, 24, 97, 49]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[91, 98, 96, 97,  6]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[85, 98, 66, 97, 81]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[10, 98, 46, 97, 72]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 2, 98, 94, 97, 91]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[30, 98, 25, 97, 71]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[40, 98, 30, 97, 36]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[67, 98, 10, 97, 88]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 4, 98, 95, 97, 89]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[80, 98, 65, 97, 59]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[10, 98, 92, 97, 47]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[84, 98, 53, 97, 87]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[75, 98, 67, 97, 78]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[43, 98, 55, 97, 37]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[33, 98, 86, 97, 25]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[18, 98, 39, 97, 23]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[66, 98, 49, 97, 33]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[31, 98,  1, 97, 31]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[67, 98, 78, 97, 85]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[57, 98, 81, 97, 58]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[40, 98, 80, 97, 96]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[75, 98, 38, 97, 37]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[17, 98, 85, 97, 87]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 2, 98, 41, 97, 82]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[93, 98, 38, 97, 42]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[28, 98, 54, 97, 57]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[76, 98, 11, 97, 60]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[95, 98, 81, 97, 32]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[21, 98, 88, 97,  5]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[52, 98,  3, 97, 59]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[14, 98, 91, 97, 13]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[52, 98, 50, 97, 78]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[47, 98,  1, 97, 47]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[75, 98, 20, 97, 45]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[63, 98, 80, 97, 93]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[23, 98, 50, 97, 83]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 1, 98, 52, 97, 52]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 4, 98,  8, 97, 32]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[22, 98, 96, 97, 75]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 2, 98, 88, 97, 79]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[82, 98, 10, 97, 44]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[68, 98,  5, 97, 49]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[56, 98, 50, 97, 84]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[ 7, 98, 14, 97,  1]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[17, 98, 32, 97, 59]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[95, 98, 96, 97,  2]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[18, 98, 18, 97, 33]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[25, 98, 34, 97, 74]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[64, 98, 58, 97, 26]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[87, 98, 74, 97, 36]])\n",
      "    )\n",
      "), GradTrackingTensor(lvl=2, value=\n",
      "    BatchedTensor(lvl=1, bdim=1, value=\n",
      "        tensor([[68, 98, 55, 97, 54]])\n",
      "    )\n",
      "))\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "too many values to unpack (expected 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[40], line 20\u001b[0m\n\u001b[1;32m     12\u001b[0m attributor \u001b[38;5;241m=\u001b[39m IFAttributor(\n\u001b[1;32m     13\u001b[0m     target_func\u001b[38;5;241m=\u001b[39mf,\n\u001b[1;32m     14\u001b[0m     params\u001b[38;5;241m=\u001b[39mmodel_params,\n\u001b[1;32m     15\u001b[0m     ihvp_solver\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcg\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     16\u001b[0m     ihvp_kwargs\u001b[38;5;241m=\u001b[39m{\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_iter\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m10\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mregularization\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m1e-2\u001b[39m},\n\u001b[1;32m     17\u001b[0m )\n\u001b[1;32m     19\u001b[0m attributor\u001b[38;5;241m.\u001b[39mcache(train_loader)\n\u001b[0;32m---> 20\u001b[0m score \u001b[38;5;241m=\u001b[39m \u001b[43mattributor\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mattribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtrain_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_loader\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/dataval/dattri/dattri/algorithm/influence_function.py:166\u001b[0m, in \u001b[0;36mIFAttributor.attribute\u001b[0;34m(self, train_dataloader, test_dataloader)\u001b[0m\n\u001b[1;32m    154\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m train_batch_idx, train_batch_data_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    155\u001b[0m     tqdm(\n\u001b[1;32m    156\u001b[0m         train_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    160\u001b[0m ):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;66;03m# get gradient of train\u001b[39;00m\n\u001b[1;32m    162\u001b[0m     train_batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    163\u001b[0m         data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m train_batch_data_\n\u001b[1;32m    164\u001b[0m     )\n\u001b[0;32m--> 166\u001b[0m     train_batch_grad \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_func\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparams\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrain_batch_data\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    168\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m test_batch_idx, test_batch_data_ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(\n\u001b[1;32m    169\u001b[0m         tqdm(\n\u001b[1;32m    170\u001b[0m             test_dataloader,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    174\u001b[0m     ):\n\u001b[1;32m    175\u001b[0m         \u001b[38;5;66;03m# get gradient of test\u001b[39;00m\n\u001b[1;32m    176\u001b[0m         test_batch_data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mtuple\u001b[39m(\n\u001b[1;32m    177\u001b[0m             data\u001b[38;5;241m.\u001b[39mto(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m0\u001b[39m) \u001b[38;5;28;01mfor\u001b[39;00m data \u001b[38;5;129;01min\u001b[39;00m test_batch_data_\n\u001b[1;32m    178\u001b[0m         )\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/apis.py:201\u001b[0m, in \u001b[0;36mvmap.<locals>.wrapped\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapped\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 201\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mvmap_impl\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43min_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mchunk_size\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/vmap.py:331\u001b[0m, in \u001b[0;36mvmap_impl\u001b[0;34m(func, in_dims, out_dims, randomness, chunk_size, *args, **kwargs)\u001b[0m\n\u001b[1;32m    320\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _chunked_vmap(\n\u001b[1;32m    321\u001b[0m         func,\n\u001b[1;32m    322\u001b[0m         flat_in_dims,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    327\u001b[0m         \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs,\n\u001b[1;32m    328\u001b[0m     )\n\u001b[1;32m    330\u001b[0m \u001b[38;5;66;03m# If chunk_size is not specified.\u001b[39;00m\n\u001b[0;32m--> 331\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_flat_vmap\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    332\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    333\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    334\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_in_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    335\u001b[0m \u001b[43m    \u001b[49m\u001b[43mflat_args\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    336\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs_spec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    337\u001b[0m \u001b[43m    \u001b[49m\u001b[43mout_dims\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    338\u001b[0m \u001b[43m    \u001b[49m\u001b[43mrandomness\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    339\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    340\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/vmap.py:480\u001b[0m, in \u001b[0;36m_flat_vmap\u001b[0;34m(func, batch_size, flat_in_dims, flat_args, args_spec, out_dims, randomness, **kwargs)\u001b[0m\n\u001b[1;32m    476\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m vmap_increment_nesting(batch_size, randomness) \u001b[38;5;28;01mas\u001b[39;00m vmap_level:\n\u001b[1;32m    477\u001b[0m     batched_inputs \u001b[38;5;241m=\u001b[39m _create_batched_inputs(\n\u001b[1;32m    478\u001b[0m         flat_in_dims, flat_args, vmap_level, args_spec\n\u001b[1;32m    479\u001b[0m     )\n\u001b[0;32m--> 480\u001b[0m     batched_outputs \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mbatched_inputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    481\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m _unwrap_batched(batched_outputs, out_dims, vmap_level, batch_size, func)\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/apis.py:397\u001b[0m, in \u001b[0;36mgrad.<locals>.wrapper\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    396\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mwrapper\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[0;32m--> 397\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43meager_transforms\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgrad_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1451\u001b[0m, in \u001b[0;36mgrad_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1450\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mgrad_impl\u001b[39m(func: Callable, argnums: argnums_t, has_aux: \u001b[38;5;28mbool\u001b[39m, args, kwargs):\n\u001b[0;32m-> 1451\u001b[0m     results \u001b[38;5;241m=\u001b[39m \u001b[43mgrad_and_value_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfunc\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margnums\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhas_aux\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1452\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1453\u001b[0m         grad, (_, aux) \u001b[38;5;241m=\u001b[39m results\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/vmap.py:48\u001b[0m, in \u001b[0;36mdoesnt_support_saved_tensors_hooks.<locals>.fn\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     45\u001b[0m \u001b[38;5;129m@functools\u001b[39m\u001b[38;5;241m.\u001b[39mwraps(f)\n\u001b[1;32m     46\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfn\u001b[39m(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs):\n\u001b[1;32m     47\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mautograd\u001b[38;5;241m.\u001b[39mgraph\u001b[38;5;241m.\u001b[39mdisable_saved_tensors_hooks(message):\n\u001b[0;32m---> 48\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/grok/lib/python3.10/site-packages/torch/_functorch/eager_transforms.py:1409\u001b[0m, in \u001b[0;36mgrad_and_value_impl\u001b[0;34m(func, argnums, has_aux, args, kwargs)\u001b[0m\n\u001b[1;32m   1406\u001b[0m diff_args \u001b[38;5;241m=\u001b[39m _slice_argnums(args, argnums, as_tuple\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[1;32m   1407\u001b[0m tree_map_(partial(_create_differentiable, level\u001b[38;5;241m=\u001b[39mlevel), diff_args)\n\u001b[0;32m-> 1409\u001b[0m output \u001b[38;5;241m=\u001b[39m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1410\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_aux:\n\u001b[1;32m   1411\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28misinstance\u001b[39m(output, \u001b[38;5;28mtuple\u001b[39m) \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(output) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m2\u001b[39m):\n",
      "File \u001b[0;32m~/dataval/dattri/dattri/func/utils.py:279\u001b[0m, in \u001b[0;36mflatten_func.<locals>.flatten_params_wrapper.<locals>._function_flattened\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m    277\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    278\u001b[0m     new_args[param_num] \u001b[38;5;241m=\u001b[39m _unflatten_params(args[param_num], model)\n\u001b[0;32m--> 279\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunction\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_args\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[40], line 6\u001b[0m, in \u001b[0;36mf\u001b[0;34m(params, data_target_pair)\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;129m@flatten_func\u001b[39m(model)\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mf\u001b[39m(params, data_target_pair):\n\u001b[1;32m      5\u001b[0m     \u001b[38;5;28mprint\u001b[39m(data_target_pair)\n\u001b[0;32m----> 6\u001b[0m     x, y \u001b[38;5;241m=\u001b[39m data_target_pair\n\u001b[1;32m      7\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nn\u001b[38;5;241m.\u001b[39mCrossEntropyLoss()\n\u001b[1;32m      8\u001b[0m     yhat \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mfunc\u001b[38;5;241m.\u001b[39mfunctional_call(model, params, x)\n",
      "\u001b[0;31mValueError\u001b[0m: too many values to unpack (expected 2)"
     ]
    }
   ],
   "source": [
    "# model = train_mnist_lr(train_loader)\n",
    "\n",
    "@flatten_func(model)\n",
    "def f(params, data_target_pair):\n",
    "    print(data_target_pair)\n",
    "    x, y = data_target_pair\n",
    "    loss = nn.CrossEntropyLoss()\n",
    "    yhat = torch.func.functional_call(model, params, x)\n",
    "    return loss(yhat, y)\n",
    "\n",
    "model_params = {k: p for k, p in model.named_parameters() if p.requires_grad}\n",
    "attributor = IFAttributor(\n",
    "    target_func=f,\n",
    "    params=model_params,\n",
    "    ihvp_solver=\"cg\",\n",
    "    ihvp_kwargs={\"max_iter\": 10, \"regularization\": 1e-2},\n",
    ")\n",
    "\n",
    "attributor.cache(train_loader)\n",
    "score = attributor.attribute(train_loader, test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "grok",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
